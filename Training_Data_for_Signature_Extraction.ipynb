{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Data for Signature Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_8txBLwr8GD"
      },
      "source": [
        "This Colab uses the [BC3: British Columbia Conversation Corpora](https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html) to generate a training dataset for Google Cloud Vertex AI Entity Extraction to train an email signature extraction model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY4y-AHAsOrv"
      },
      "source": [
        "## Load Python Modules\n",
        "First, let's load some packages to help with parsing the XML file provided by the University of British Columbia\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DWLAZ4Ir0Dj",
        "outputId": "0aa3bb35-3c39-462f-b296-053687061229"
      },
      "source": [
        "! pip install bs4 lxml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOUaONhlr7O4"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import lxml\n",
        "import html\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import json"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPCd_vrosfL4"
      },
      "source": [
        "## Download Email Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23C5uAW5sDXX",
        "outputId": "b2250616-19c1-475c-f94a-331b8d48723e"
      },
      "source": [
        "! wget https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3/bc3.1.0.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 22:06:59--  https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3/bc3.1.0.zip\n",
            "Resolving www.cs.ubc.ca (www.cs.ubc.ca)... 142.103.6.5\n",
            "Connecting to www.cs.ubc.ca (www.cs.ubc.ca)|142.103.6.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124299 (121K) [application/zip]\n",
            "Saving to: ‘bc3.1.0.zip’\n",
            "\n",
            "bc3.1.0.zip         100%[===================>] 121.39K   519KB/s    in 0.2s    \n",
            "\n",
            "2021-09-03 22:07:00 (519 KB/s) - ‘bc3.1.0.zip’ saved [124299/124299]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMJlRee7s2AA",
        "outputId": "3e255da4-8fcf-4696-a5f5-a6c2501839b4"
      },
      "source": [
        "! unzip bc3.1.0.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bc3.1.0.zip\n",
            "  inflating: annotation.dtd          \n",
            "  inflating: annotation.xml          \n",
            "  inflating: corpus.dtd              \n",
            "  inflating: corpus.xml              \n",
            "  inflating: README.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIIPJ7FuJ5E"
      },
      "source": [
        "## Load Email Data from XML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56OmIc6fs7-e"
      },
      "source": [
        "with open(\"./corpus.xml\", \"r\") as file:\n",
        "  soup = BeautifulSoup(file, \"lxml\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33F_3EusuRoy"
      },
      "source": [
        "# regex to remove prior thread\n",
        "regex = re.compile(r'On .* wrote: .*', flags=re.DOTALL)\n",
        "email_soup = soup.find_all('text')\n",
        "emails = []\n",
        "\n",
        "for email in email_soup:\n",
        "  # email_text = email.text.replace(\"\\n\",\" \") # remove new lines\n",
        "  email_text = re.sub(regex, '', email.text) # remove prior thread\n",
        "  emails.append(html.unescape(email_text))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n08IyrOu-0nU"
      },
      "source": [
        "## Write Email Data to JSONL\n",
        "Vertex AI will allow you to import a jsonl file as training data automatically, and it also handles the train/test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu8YXkfA7wMK"
      },
      "source": [
        "with open(f'./bc3_emails.jsonl', 'w') as outfile:\n",
        "  for e in emails:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"textContent\": e\n",
        "        }, outfile)\n",
        "    outfile.write('\\n')"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}